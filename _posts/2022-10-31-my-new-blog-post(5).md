## Fourth Blog Post!

Prompt: 

When building regression models, it often takes a lot of experience and knowledge about your data in order to determine the variables and transformation of variables that you want to include in the model building process.  There are many variable selection techniques (or feature selection) but it can be a confusing practice when you are first learning. **Write up a brief discussion of how you would plan to determine variables to use in a regression model.  What variable selection techniques do you prefer and why?**

I don't have much experience with regression modeling, but I'll give my opinion based on what I learned from the assigned reading. There are many things to keep in mind regarding regression. (1) It is important that we explain the data in the simplest way, with redundant predictors removed. (2) Unecessary predictors will add noise and waste the degrees of freedom. (3) Collinearity is caused by having too many variables trying to do the same job. Prior to variable selection, we need to indentify any outliers and influential points as well as add in any transformations of the variables that seem appropriate. 

There seems to be two extremes with variable selection: a model with the best predictor variable and a model with all predictor variables. The problem with regression modeling is trying to find the necessary variables among the complete set variables by deleting both irrelevant variables and redundant variables. Two popular variable selection methods are forward selection (FS) and backward elimination (BE). These models either add or delete variables one by one to find the best model. These aren't necessarily the best methods as you may end up with irrelevant or redundant variables in your model. The next best thing is stepwise selection, which is a modification of forward selection. It differs because variables that are in the model already do not necessarily stay. This seems better than FS because variables can be removed from the model if deemed unecessary. 

